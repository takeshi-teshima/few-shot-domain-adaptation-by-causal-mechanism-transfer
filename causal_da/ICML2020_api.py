from pathlib import Path
from typing import Iterable, Dict, Optional, Union, List
import numpy as np
import torch
from sklearn.model_selection import ParameterGrid
from .components.ica_torch.api import GCLTrainableInvertibleICAModel
from .components.inn_torch import GlowNonExponential
from .algorithm.api import CausalMechanismTransfer
from .api_support.index_class_label import index_class_label
from .api_support.logging import DummyRunLogger, MongoAndSacredRunLogger, ParamHistoryManagerBase
from .api_support.logging.model_logger import MLFlowModelLogger
from .components.aug_predictor import AugKRR

# Type hinting
from .api_support.logging import RunLogger
from .api_support.evaluator_runner import EvaluatorRunner
from .api_support.evaluator import AugmenterEvaluatorBase, AugmenterSavingEvaluator


class _RunWrapper:
    """A wrapper class to hold the unchanged arguments throughout all runs of Sacred."""
    def __init__(self, func, static_args, static_kwargs):
        """
        Parameters:
            func: the function to be wrapped.
            static_args: the static arguments.
            static_kwargs: the static keyword arguments.
        """
        self.func = func
        self.static_args = static_args
        self.static_kwargs = static_kwargs

    def __call__(self, idx, params_injector, run_logger):
        """Run the function with the saved parameters."""
        return self.func(idx, params_injector, run_logger, *self.static_args,
                         **self.static_kwargs)


class _ICML2020APISingleRun:
    """Perform a single run and record the results."""
    def __call__(self, idx: int, params_injector: dict, run_logger: RunLogger,
                 X_src, Y_src, c_src, save_model_path: Path,
                 ica_intermediate_evaluators, augmenter_evaluators,
                 train_params):
        """Run Causal DA and record the performance for a single hyper-parameter setting.

        Parameters
        ----------
        idx : ``int``
            A unique ID of the experiment run. Used to identify the saved model file.
            Supposed to be the value generated by ``perform_run()`` of a ``MongoAndSacredRunLogger`` instance.

        params_injector : ``dict``
            A dict containing a single set (i.e., a single grid cell in a grid-search)
            of hyper-parameters to be used in this run. ``get()`` is used to fetch the params.

        run_logger

        X_src

        Y_src

        c_src

        save_model_path

        ica_intermediate_evaluators

        augmenter_evaluators

        train_params : ``dict``
        """
        data_src = np.hstack((X_src, Y_src))
        c_src = index_class_label(c_src)
        dim = data_src.shape[1]
        n_label = len(np.unique(c_src, return_index=True)[1])
        save_model_path = str(save_model_path / str(idx))

        # 0. Prepare recording.
        run_logger.start_run()
        _model_logger = MLFlowModelLogger(save_model_path, 'final_model',
                                          run_logger)
        ica_final_evaluators = EvaluatorRunner([])
        augmenter_final_evaluators = [
            AugmenterSavingEvaluator(_model_logger, 'final_model', run_logger)
        ]
        ica_run_logger = run_logger

        # 1. Build the INN model to be used inside the ICA model.
        inn = self._get_inn(params_injector, dim, run_logger)

        # 2. Build the out-of-box ICA model to be passed to our method.
        invertible_ica_model = self._prepare_trainable_ica_model(
            inn, params_injector, dim, n_label, run_logger, train_params)

        # 3. Run the method and record the results.
        predictor_model = AugKRR()
        method = CausalMechanismTransfer(invertible_ica_model, predictor_model)
        ica_data = (data_src, c_src)

        method.train_and_record(data_src, ica_data, ica_run_logger,
                                ica_intermediate_evaluators,
                                ica_final_evaluators, augmenter_evaluators,
                                augmenter_final_evaluators)

    def _get_inn(self, params_injector: dict, dim: int, run_logger):
        """Build the invertible neural network model.

        Parameters:
            params_injector: the parameter holder.
            dim: the data dimension.
            run_logger: the logger to save the parameters.
        """
        depth = params_injector.get('depth')
        n_hidden = params_injector.get('n_hidden')
        inn = GlowNonExponential(depth=depth, dim=dim, n_hidden=n_hidden)
        run_logger.log_params({
            'depth': depth,
            'n_hidden': n_hidden,
        })
        return inn

    def _prepare_trainable_ica_model(self, inn, params_injector, dim, n_label,
                                     run_logger, train_params):
        """Build the trainable invertible ICA model based on the invertible neural network.

        Parameters:
            inn: the invertible neural network model.
            params_injector: the parameter holder.
            dim: the data dimension.
            n_label: the number of the source domains.
            run_logger: the logger to save the parameters.
            train_params: the training-related parameters passed to the trainable ICA model.
        """
        classifier_hidden_dim = params_injector.get('classifier_hidden_dim')
        classifier_n_layer = params_injector.get('classifier_n_layer')
        run_logger.log_params({
            'classifier_n_layer': classifier_n_layer,
            'classifier_hidden_dim': classifier_hidden_dim,
        })
        ica_model = GCLTrainableInvertibleICAModel(inn, dim,
                                                   classifier_hidden_dim,
                                                   n_label, classifier_n_layer)
        lr = params_injector.get('lr')
        weight_decay = params_injector.get('weight_decay')
        device = train_params['device']
        batch_size = train_params.get('batch_size')
        max_epochs = train_params.get('epochs')
        ica_model.set_train_params(lr, weight_decay, device, batch_size,
                                   max_epochs)
        return ica_model


class CausalMechanismTransferICML2020API:
    """The main interface class for the experiments. Based on the following combination of components:

    * Hyper-paramter search: grid-search.

        * HP selection is assumed to be performed by recording the validation scores for all runs in MongoDB and comparing the scores later.

    * Experiment recording: Sacred + MongoDB.

    * Model recording: MLFlow.

    * ICA

        * training: GCL (out-of-the-box).

        * Invertible neural network: Glow-based (without exponential activation).

    * Predictor

        * Kernel ridge regression (RBF kernel)
    """
    def __init__(
            self,
            run_logger: Optional[MongoAndSacredRunLogger] = None,
            param_history_manager: Optional[ParamHistoryManagerBase] = None):
        """Construct the method object.
        """
        if run_logger is None:
            self.run_logger = DummyRunLogger()
        else:
            self.run_logger = run_logger
        self.param_history_manager = param_history_manager

    def _prepare_hyperparam_grid(self, cfg_method: dict):
        """Prepare the hyperparameter grid by taking all combinations.

        Parameters:
            cfg_method: the parameter holder.
        """
        space = cfg_method['base_param']
        space.update(cfg_method['model_param'])
        param_grid = list(ParameterGrid(space))
        return param_grid

    def _split_and_filter_hyperparam_candidates(self, param_grid,
                                                parallel_split_index,
                                                total_parallel_split):
        """
        Parameters
        ----------
        parallel_split_index : ``int``
            We use brute-force parallelization such that we split the set of
            all hyperparameter configurations into ``total_parallel_split`` chunks.

        total_parallel_split : ``int``
            The total number of hyper-parameter splits for parallelization. See also ``parallel_split_index``.
        """
        param_grid = np.array_split(
            param_grid, total_parallel_split)[parallel_split_index - 1]
        if self.param_history_manager is not None:
            param_grid = self.param_history_manager.filter(param_grid)
        return param_grid

    def run_method_and_eval(self,
                            X_src,
                            Y_src,
                            c_src,
                            cfg_method,
                            parallel_split_index: int,
                            total_parallel_split: int,
                            save_model_path: Path,
                            ica_intermediate_evaluators: EvaluatorRunner,
                            augmenter_evaluators: List[AugmenterEvaluatorBase],
                            train_params=dict(epochs=500,
                                              batch_size=32,
                                              device='cpu'),
                            debug: bool = False):
        """Main interface.
        Runs through all the hyper-paramter candidates except those for which we have a previous run record.
        Designed for training, probing the performance, and saving the results in a database that is connected to Sacred run database (via sacred's ``run_id``), for all hyper-parameter combinations.

        Parameters
        ----------
        X_src : ``numpy.ndarray``
            Source domain input data (shape ``(n_data, n_dim)``).

        Y_src : ``numpy.ndarray``
            Source domain predicted variable data (shape ``(n_data, 1)``).

        c_src : ``numpy.ndarray``
            Source domain index (shape ``(n_data,)``).

        cfg_method : ``dict``
            The keys should contain ``('base_param', 'model_param')``.
            Each entry (inside each of these keys) should be lists.
            The hyper-parameter grid will be generated by taking the products of these lists.

        parallel_split_index : ``int``
            Used in ``_split_and_filter_hyperparam_candidates()`` to enable brute-force parallelization.

        total_parallel_split : ``int``
            Used in ``_split_and_filter_hyperparam_candidates()`` to enable brute-force parallelization.

        save_model_path : ``pathlib.Path``
            The base path used for saving the augmenter model
            (this is used only as a base path and the experiment run ID will be appended).

        ica_intermediate_evaluators : ``EvaluatorRunner``
            An evaluator runner used during the ICA training loop.

        augmenter_evaluators : ``List[AugmenterEvaluatorBase]``
            A list of evaluators that depend on the augmenter and are run during the ICA training loop.

        train_params : ``dict``
            Contains ``(epochs, batch_size, device)`` as the keys.
            Keys:

            * ``device`` : The device specification (e.g., ``'gpu'``) for training in PyTorch.

        debug : ``bool``
            Debug mode (default ``False``). If false, exceptions are not caught during the loop.
        """
        # Generate hyper-parameter grid.
        param_grid = self._prepare_hyperparam_grid(cfg_method)
        # Split the grid cells for brute-force parallelization.
        param_grid = self._split_and_filter_hyperparam_candidates(
            param_grid, parallel_split_index, total_parallel_split)

        # Run the train-eval loop.
        args = X_src, Y_src, c_src, save_model_path, ica_intermediate_evaluators, augmenter_evaluators, train_params
        self.single_run_wrapper = _RunWrapper(_ICML2020APISingleRun(), args,
                                              dict())
        for params in param_grid:
            if debug:
                self.run_logger.perform_run(
                    lambda idx, _params: self.single_run_wrapper(
                        idx, _params, run_logger=self.run_logger), params)
            else:
                try:
                    self.run_logger.perform_run(
                        lambda idx, _params: self.single_run_wrapper(
                            idx, _params, run_logger=self.run_logger), params)
                except Exception as err:
                    print(err)
